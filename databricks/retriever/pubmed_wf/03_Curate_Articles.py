# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC # Curate Articles
# MAGIC
# MAGIC This notebook will run a first level parsing and save article content into a delta table. This step of saving files to an intermediate table before further processing is desired because it consolidates many articles into a single file and removes the first level of the element tree. Both improve performance for iterative use.
# MAGIC
# MAGIC This task will append new articles in `pudbmed_wf.curated_articles_xml` table. If a document will be updated, a new AccessionID/PMID will be generated by PMC and the old one will be retracted.
# MAGIC
# MAGIC **TODO**: Add redactions 

# COMMAND ----------

# DBTITLE 1,Initialize pubmed_wf Application Class
# MAGIC %run ./_setup/setup_pubmed_wf $SHOW_TABLE=false $SHOW_WORKFLOW=true

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC The `curated_articles_xml` table gets it's DDL from <a href="$../../_config/ddl/CREATE_TABLE_curated_articles_xml.sql" target="_blank">CREATE_TABLE_curated_articles_xml.sql</a>.

# COMMAND ----------

# curated_articles_xml has inner XML for fields front, body, floatgroups, back, and processing_metadata

sql_path = pubmed_wf.curated_articles_xml.sql_path
with open(sql_path, 'r') as file:
    sql = file.read()
    print(sql)

# COMMAND ----------

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, IntegerType, DoubleType, MapType, StructType, StructField

# Define the UDF
def article_parse_xml(accession_id: str, volume_path: str) -> tuple:
    from bs4 import BeautifulSoup
    with open(volume_path, 'r') as file:
        article = BeautifulSoup(file.read(), 'xml').find('article')
    return (accession_id, volume_path,
        {str(k):str(v) for k,v in article.attrs.items()},
        str(article.find('front')),
        str(article.find('body')),
        str(article.find('floats-group')),
        str(article.find('back')),
        # TODO: update processing metadata type to map
        str(article.find('processing-meta')))

# Register the UDF
article_parse_xml_udf = udf(article_parse_xml, 
                            returnType=StructType([StructField("AccessionID",         StringType(), nullable=False),
                                                   StructField("volume_path",         StringType(), nullable=False),
                                                   StructField("attrs",               MapType(StringType(), StringType()), nullable=False),
                                                   StructField("front",               StringType(), nullable=False),
                                                   StructField("body",                StringType(), nullable=False),
                                                   StructField("floats_group",        StringType(), nullable=False),
                                                   StructField("back",                StringType(), nullable=False),
                                                   StructField("processing_metadata", StringType(), nullable=False)]))

# COMMAND ----------

parsed_articles = pubmed_wf.raw_metadata_xml.df.filter('status="DOWNLOADED"') \
                        .join(pubmed_wf.curated_articles_xml.df, 'AccessionID', 'leftanti') \
                        .withColumn('parsed_struct', article_parse_xml_udf("AccessionID", "volume_path")) \
                        .select('parsed_struct.AccessionID',
                                'ETag',
                                'LastUpdated',
                                'PMID',
                                'parsed_struct.attrs',
                                'parsed_struct.front',
                                'parsed_struct.body',
                                'parsed_struct.floats_group',
                                'parsed_struct.back',
                                'parsed_struct.processing_metadata',
                                '_ingestion_timestamp',
                                'volume_path')

# COMMAND ----------

merged_parsed_articles = pubmed_wf.curated_articles_xml.dt.alias('tgt') \
                                  .merge(parsed_articles.alias('src'), "src.AccessionID = tgt.AccessionID") \
                                  .whenMatchedUpdateAll() \
                                  .whenNotMatchedInsertAll() \
                                  .execute()
